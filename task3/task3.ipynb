{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 用pytorch实现逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training, predict of x = 1.5 is:\ny_pred =  0.0\nafter training, predict of x = 1.5 is:\ny_pred = 0.0\ntensor([0.6075]) tensor([-0.9949])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.0], [0.0], [1.0], [1.0]]))\n",
    "\n",
    "#初始化\n",
    "w = Variable(torch.Tensor([-1]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([0]), requires_grad=True)\n",
    "epochs = 100\n",
    "costs = []\n",
    "lr = 0.1\n",
    "print(\"before training, predict of x = 1.5 is:\")\n",
    "print(\"y_pred = \", float(w.data*1.5 + b.data > 0))\n",
    "\n",
    "#模型训练\n",
    "for epoch in range(epochs):\n",
    "\t#计算梯度\n",
    "\tA = 1/(1+torch.exp(-(w*x_data+b))) #逻辑回归函数\n",
    "\tJ = -torch.mean(y_data*torch.log(A) + (1-y_data)*torch.log(1-A))  #逻辑回归损失函数\n",
    "\t#J = -torch.mean(y_data*torch.log(A) + (1-y_data)*torch.log(1-A)) +alpha*w**2\n",
    "\t#基础类进行正则化，加上L2范数\n",
    "\tcosts.append(J.data)\n",
    "\tJ.backward()  #自动反向传播\n",
    "\n",
    "\t#参数更新\n",
    "\tw.data = w.data - lr*w.grad.data\n",
    "\tw.grad.data.zero_()\n",
    "\tb.data = b.data - lr*b.grad.data\n",
    "\tb.grad.data.zero_()\n",
    "\n",
    "print(\"after training, predict of x = 1.5 is:\")\n",
    "print(\"y_pred =\", float(w.data*1.5+b.data > 0))\n",
    "print(w.data, b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用pytorch实现torch.nn.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training)given 4 is 0.0\nepoch =  1 <built-in method item of Tensor object at 0x0000000008813168>\nepoch =  2 <built-in method item of Tensor object at 0x00000000088130D8>\nepoch =  3 <built-in method item of Tensor object at 0x00000000088131F8>\nepoch =  4 <built-in method item of Tensor object at 0x0000000008813240>\nepoch =  5 <built-in method item of Tensor object at 0x00000000088131B0>\nepoch =  6 <built-in method item of Tensor object at 0x0000000008813168>\nepoch =  7 <built-in method item of Tensor object at 0x00000000088130D8>\nepoch =  8 <built-in method item of Tensor object at 0x00000000088131F8>\nepoch =  9 <built-in method item of Tensor object at 0x0000000008813240>\nepoch =  10 <built-in method item of Tensor object at 0x0000000008813168>\nepoch =  11 <built-in method item of Tensor object at 0x0000000008813288>\nepoch =  12 <built-in method item of Tensor object at 0x00000000088130D8>\nepoch =  13 <built-in method item of Tensor object at 0x00000000088131F8>\nepoch =  14 <built-in method item of Tensor object at 0x0000000008813240>\nepoch =  15 <built-in method item of Tensor object at 0x0000000008813168>\nepoch =  16 <built-in method item of Tensor object at 0x0000000008813288>\nepoch =  17 <built-in method item of Tensor object at 0x00000000088130D8>\nepoch =  18 <built-in method item of Tensor object at 0x00000000088131F8>\nepoch =  19 <built-in method item of Tensor object at 0x0000000008813240>\nepoch =  20 <built-in method item of Tensor object at 0x0000000008813168>\nepoch =  21 <built-in method item of Tensor object at 0x0000000008813288>\nepoch =  22 <built-in method item of Tensor object at 0x00000000088130D8>\nepoch =  23 <built-in method item of Tensor object at 0x00000000088131F8>\nepoch =  24 <built-in method item of Tensor object at 0x0000000008813240>\nepoch =  25 <built-in method item of Tensor object at 0x0000000008813168>\nepoch =  26 <built-in method item of Tensor object at 0x0000000008813288>\nepoch =  27 <built-in method item of Tensor object at 0x00000000088130D8>\nepoch =  28 <built-in method item of Tensor object at 0x00000000088131F8>\nepoch =  29 <built-in method item of Tensor object at 0x0000000008813240>\nepoch =  30 <built-in method item of Tensor object at 0x0000000008813168>\nepoch =  31 <built-in method item of Tensor object at 0x0000000008813288>\nepoch =  32 <built-in method item of Tensor object at 0x00000000088130D8>\nepoch =  33 <built-in method item of Tensor object at 0x00000000088131F8>\nepoch =  34 <built-in method item of Tensor object at 0x0000000008813240>\nepoch =  35 <built-in method item of Tensor object at 0x0000000008813168>\nepoch =  36 <built-in method item of Tensor object at 0x0000000008813288>\nepoch =  37 <built-in method item of Tensor object at 0x00000000088130D8>\nepoch =  38 <built-in method item of Tensor object at 0x00000000088131F8>\nepoch =  39 <built-in method item of Tensor object at 0x0000000008813240>\nepoch =  40 <built-in method item of Tensor object at 0x0000000008813168>\npredict (after training)given 4 is 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.0], [0.0], [1.0], [1.0]]))\n",
    "\n",
    "#定义网络模型\n",
    "#先建立一个基类Module，都是从父类torch.nn.Module继承过来，Pytorch写网络的固定写法\n",
    "class Model(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Model, self).__init__()  #初始父类\n",
    "\t\tself.linear = torch.nn.Linear(1, 1)  #输入维度和输出维度都为1\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\ty_pred = self.linear(x)\n",
    "\t\treturn y_pred\n",
    "\n",
    "model = Model()  #实例化\n",
    "\n",
    "#定义loss和优化方法\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  #损失函数，封装好的逻辑损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)   #进行优化梯度下降\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "#Pytorch类方法正则化方法，添加一个weight_decay参数进行正则化\n",
    "\n",
    "#befor training \n",
    "hour_var = Variable(torch.Tensor([[2.5]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (before training)given\", 4, 'is', float(model(hour_var).data[0][0]>0.5))\n",
    "\n",
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "\t#计算grads和cost\n",
    "\ty_pred = model(x_data)   #x_data输入数据进入模型中\n",
    "\tloss = criterion(y_pred, y_data)\n",
    "\tprint('epoch = ', epoch+1, loss.item)\n",
    "\toptimizer.zero_grad() #梯度清零\n",
    "\tloss.backward() #反向传播\n",
    "\toptimizer.step()  #优化迭代\n",
    "\n",
    "#After training \n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (after training)given\", 4, 'is', float(model(hour_var).data[0][0]>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
